---
title: Uploading to Central Staging Data Warehouse
id: Uploading to Central Staging Data Warehouse
description:  Uploading to Central Staging Data Warehouse
---

# Purpose

The motivation of this Standard Operating Procedure (SOP) is to provide a tool to track a data generating site (DGS)'s file repository, identify changed files, and submit the changed files to the central CHoRUS cloud repository.  Two primary motivations for this SOP are

1. versioning of submitted data.  Any file from any prior submission should be accessible by CHoRUS users.  This implies all past versions of files are tracked, and a database allows query for past versions.
2. Minimize the data upload to only added and modified files.  Note that deleted DGS files are never removed from central storage.

 Versioning of the DGS-submitted data and therefore the central dataset is critical for for auditing, provenance, and reproducibility.  


This SOP specifies the expected processes for managing site data, uploading to the central CHoRUS cloud environment, the change-tracking process, and the submission process. This SOP describes
1.	DGS's local data structure
2.	DGS's local data submission package preparation process
3.	The central data structure

The SOP aims to simplify the process for submission package preparation and the actual submission.  A python based tool has been created as a reference implementation of this process, and the SOP will describe the processes in terms of application of this tool.

## Rationale
DGSs will continually accrue and update DGS-local data.  Each scheduled submission will contain
1.	New subjects
2.	Existing subjects with additional observations, as OMOP observations, NLP extracts of clinical notes, and additional images and waveforms.
3.	Corrections/replacements of existing data including OMOP observations, NLP extracts of clinical notes, images, and waveforms for individual subjects
4.	Corrections/replacements of existing data including OMOP observations, NLP extracts of clinical notes, images, and waveforms for previous submission due to coding changes, deidentification process changes, new NLP algorithms, etc.

The objective is to be able to track the changes when submitting.  The SOP is guided by the following objectives:
- Streamline the data submission process through automation
- Reduce the data submission package size

To reduce the amount of data to be transmitted, this SOP focuses on identification of changed file subset and submit only those that have been added, modified, or deleted.  This applies to images and waveforms.   As OMOP clinical tables are relatively small in size, and to simplify the data validation and merging process, the OMOP clinical tables will be submitted in their entirety in each submission.

The submitted data will be stored in the Central Staging area, which will maintain all previously submitted files, grouped by submission time stamp.


## Alternatives Considered
Alternative approaches considered for versioning include

1. Azure Blob Storage's own versioning: it does not support concurrent access of multiple data versions by multiple users.  
2. Azure Blob Storage's snapshots: snapshots are timestamped, but identifying files that belong to a time frame for public data release is difficult.
3. Azure Blob Storage soft delete: soft delete allows deleted files to be accessed, but is time limited, and does not support versioning or timestamp.
4. Git Large File Storage: git based file versioning but would require full checkout to access a past version.

Alternative approaches to minimize data movement include
1. azcopy, rsync:  these methods reduce data transmission to changed set, but do not track past history or deleted files.
2. Azure Data Share:  current approach copies files to ensure CHoRUS maintains control over submitted data; moving changed set renders it similar to rsync and azcopy. 

## Terms and abbreviations
SOP: Standard Operating Procedure
DGS: Data generating site
Local: Refers to DGS's own storage or data.  Note that this could be on-premise or in DGS's own cloud.
Staging: Refers to central CHoRUS storage or data in the DGS specific container in the Azure cloud

## Related SOPs:
- This SOP is related to procedures for DGS data preparation (ETL, transformation, deidentification, etc).  This SOP assumes data preparation has been completed and new data is ready to be submitted.
- This SOP is related to the SOPs for Central Staging data validation and Central Data Merging.  This SOP does not include procedures from the central data integration SOP.


# Tooling and Automation

The change tracking and submission tool is maintained in the [chorus-extract-upload](https://github.com/chorus-ai/chorus-extract-upload) repository.  The initial data submission can occur before the tooling is formally released, in which case it should follow the “Interim Data Upload Procedure” instruction outlined below.

## Prerequisites
The chorus-extract-upload tool has the following assumptions
1. The DGS has a local file system or cloud storage where the data is stored.  A DGS may use different services for each of the data modalities, e.g. images in AWS and waveform and OMOP files in local file system.
2. The DGS will maintain all files ever submitted to CHoRUS in the local file system or cloud storage.  The tool uses the full file set to identify changed files since last upload.
3. The DGS will organize the files using the folder structure as described in the “Site-Local Folder Structure” section below.

Azure Data Share is not supported, as it does not ensure data persistence for staged data, and would complicate file versioning.

## File Organization

This SOP and the associated tool scans a DGS's data folder structure to generate a journal (aka manifest) as an SQLite file.  Each scan updates the SQLite with additions, deletions, and modifications, and tracks the time of the change. 

The journal is used to identify new and changed files to upload to central.  The journal file is copied to cental during each upload.

Since a DGS may have separate repositories for each data type, to ensure consistency, the Central cloud container contains the most current journal database file, which is downloaded prior to any upload operations.

### DGS-Local Folder Structure

The DGS-local Folder Structure should be organized into the structure as shown below.  The Update tool expects the organizational structure below for identifying the changed file sets:

![Local Data Organization](https://github.com/chorus-ai/Chorus_SOP/blob/review-data-upload-update/sop-website/docs/Data-Upload-Update/Local_Structure.png?raw=true)


Each DGS should maintain all submitted data in the folder structure shown above. Files removed from the folder structure will be considered "deleted".  The root folder should contain an `OMOP` folder, and patient folders named by DGS's de-identified patient IDs.  Each patient directoryshould contain an `Images` folder and a `Waveforms` folder.  

### OMOP EHR Folder
This folder should include structured and unstructured 13 OMOP clinical data files(exluding specimen table) as found here (https://ohdsi.github.io/CommonDataModel/cdm54.html#Clinical_Data_Tables). We are not requiring actual notes to be included in the "note_text" field of the NOTE table, but since working with the NOTE_NLP table would greatly benefit from access to foreign keys only present in the NOTE table. Thus the note table should at least contain core identifiers (note_id), descriptors (note_type_concept_id and note_class_concept_id) as well as there foreign keys to PERSON, VISIT_OCURRENCE, VISIT_DETAILS. Each of these table will be in the form of comma delimited files with header rows.

Each OMOP file should container records for ALL ACTIVE patients submitted to CHoRUS to date. The OMOP files is a snapshot of current DGS local CHoRUS OMOP tables.

### Images Folder
The `Images` folder should contain all images for the patient, with images organized in standard DICOM hierarchy as study/series folders.

File names should follow the format below:

* Patient Identification: Typically includes a person ID.
* Study Date: The date when the study was conducted, usually in the format YYYYMMDD.
* Study Time: The time of the study, often in HHMMSS format.
* Modality: Refers to the type of equipment used for the scan, such as MR (Magnetic Resonance), CT (Computed Tomography), or US (Ultrasound).
* Series Number: Indicates the sequence of a particular series of images in a study.
* Instance Number: Represents the specific image number within a series.

Example of a DICOM Image Name
```
PersonID_20230101_101530_CT_01_001.dcm
```

This example would represent a CT scan for a patient, conducted on January 1, 2023, at 10:15:30. This image is the first in its instances and the first in that series. Make sure instance and series match with dicom metadata.

### Waveforms Folder

The `Waveforms` folder should contain all waveforms for the patient organized into session folders, each session containing a set of files corresponding to a continuous recording session. 

Waveform data refers to all data acquired from bedside monitors and devices, including alarms, numerics data obtain at regular of irregular intervals, and high-frequency (>1 Hz) waveform data such as digitized EKG tracings. The waveforms should be deidentified.

Files should be named using the following format:

* Patient Identification: A unique person ID, typically a number or numeric digits from OMOP Person Table.
* Start Date: The date when the waveform data was recorded, in the format YYYYMMDD.
* Start Time: The start time of the recording, in HHMMSS format.
* Duration in Seconds: The duration of the recording in seconds.

Example File Name
```
1000001_20230101_101530_3600.h5
```

This example represents a recording for patient 1000001 on January 1, 2023, starting at 10:15:30, with a duration of 3600 seconds.


### Local Folder Structure

Example folder and file structure for a single patient is shown below:
```
10001001
  ├── Images
      ├── 1.2.840.113619.2.176.3596.6358730.30068.1592383800.1.1
          ├── 1.2.840.113619.2.176.3596.6358730.30068.1592383800.2.1
              ├── 10001001_20220401_113120_CT_01_0.dcm
              ├── 10001001_20220401_113120_CT_01_1.dcm
              ├── 10001001_20220401_113120_CT_01_2.dcm
              ...
  ├── Waveforms
      ├── 10001001_20220401_113120
          ├── 10001001_20220401_113120_3600.hea
          ├── 10001001_20220401_113120_3600_0001.dat
          ├── 10001001_20220401_113120_3600_0002.dat
          ...
```

This structure should be maintained for all subsequent submissions.  All additions, deletions, and modifications should be made in this structure.  Sites may separately maintain prior versions of OMOP files at their own discretion (we strongly recommend doing so).  A prescribed organization or naming convention is not provided.  Note also that all previously submitted OMOP files will remain in the Central Staging area.

A relaxation of the folder structure is that OMOP, images, and waveforms files may be stored in separate file systems.  For example, images may be stored in the AWS cloud, while waveforms may be stored in an on-prem Linux file system.  Each file system should maintain the patient folder level.

On AWS
```
/10001001
  ├── Images
      ├── 1.2.840.113619.2.176.3596.6358730.30068.1592383800.1.1
          ├── 1.2.840.113619.2.176.3596.6358730.30068.1592383800.2.1
              ├── 10001001_20220401_113120_CT_01_0.dcm
              ├── 10001001_20220401_113120_CT_01_1.dcm
              ├── 10001001_20220401_113120_CT_01_2.dcm
              ...
```

On local file system
```
/10001001
  ├── Waveforms
      ├── 10001001_20220401_113120
          ├── 10001001_20220401_113120_3600.hea
          ├── 10001001_20220401_113120_3600_0001.dat
          ├── 10001001_20220401_113120_3600_0002.dat
          ...
```


### Central Cloud Environment
This virtual environment is hosted in an Azure platform. The containers structure is setup such that each DGS has its own container (e.g. mgh).

![Central Data Organization](https://github.com/chorus-ai/data_acq_SOP/assets/2847495/d7d1d67a-2695-4b2d-92aa-2285b608a09d)

### Central Staging Folder Structure
The submitted data will be organized using the same folder structure as the DGS local file store, except that data for each submission will placed in a top level folder named with a timestamp of the format `YYYYMMDDhhmmss`.  The folder structure is shown here for context.  The timestamped folder will be created automatically by the Upload Tool.  

![Central Data Organization](https://github.com/chorus-ai/Chorus_SOP/blob/review-data-upload-update/sop-website/docs/Data-Upload-Update/Central_Structure.png?raw=true)

In the diagram above, the gray directories were added, blue folder has modifications, and orange folder has been deleted.

## Existing Data Upload in Different Folder Structure

Each DGS is required to organize the data files in this folder structure.  For DGSs who have previously uploaded data with a different folder structure, it is recommended that the DGS first locally organize the data and then use the Upload Tool to resubmit all existing data.

# Upload Tool

The Python based Upload Tool has been created to simplify the submission process.  The code is available in [chorus-extract-upload](https://github.com/chorus-ai/chorus-extract-upload) repository.

## Functions of the Upload Tool

The Upload Tool uses a local SQLite database to maintain manifests of files and past submissions

The Upload Tool provides the following functionalities:
1.  Create and update a file manifest by traversing the Local folders.  Additions, updates, deletions, are captured and dated 
2.  Generate list of files to upload
3.  Submit added and changed files to Central Staging area, and the manifest database.  Optionally amend a previous submission
4.  Verify past submissions against files in Central Staging
5.  Track submission history

## Installation

The Upload Tool is a Python package.  The package can be installed using pip.  The package requires Python 3.7 or later.  A virtual environment (venv or conda) is strongly recommended.

1. create and configure a conda environment
```
conda create --name chorus python=3.10.14
conda activate chorus

pip install flit
```

or alternatively with python virtual envionment
```
python -m venv {venv_directory}
source {venv_directory}/bin/activate

pip install flit
```

2. get the software
```
git clone https://github.com/chorus-ai/chorus-extract-upload
cd chorus-extract-upload
```

3. install the software and dependencies
```
flit install
```

NOTE: for developers, you can instead run 
```
flit install --symlink
```
which allows changes in the code directory to be immediately reflected in the python environment.

## Configuration File:
A `config.toml` file needs to be customized for each DGS.

```
[journal]
path = "az://container/journal.db"   # specify the journal file name. defaults to cloud storage
azure_account_name = "account_name"
azure_sas_token = "sastoken"

[central_path]
# specify the central (target) container/path, to which files are uploaded.  
# This is also the default location for the journal file
path = "az://container/"
azure_account_name = "account_name"
azure_sas_token = "sastoken"

[site_path]
  [site_path.default]
  # specify the default site (source) path
  path = "/mnt/data/site"

  # each can have its own access credentials
  [site_path.OMOP]
  # optional:  specific root paths for omop data
  path = "/mnt/data/site"

  [site_path.Images]
  # optional:  specific root paths for images
  path = "s3://container/path"
  aws_access_key_id = "access_key_id"
  aws_secret_access_key = "secret_access"

  [site_path.Waveforms]
  path = "/mnt/another_datadir/site"
```


## Usage

The Upload Tool is named `chorus_upload_journal/journal_tools` in the subdirectory `upload_manifest` in `chorus-extract-upload`.  The Upload Tool can be run as

```
python chorus_upload_journal/journal_tools [params] <subcommand> [subcommand params]
```

The `-h` parameter will display help information for the tool or each subcommand.  Suppported commands include `update`, `upload`, `usage`, and `verify`

Different `config.toml` files can be specified by using the `-c` parameter



### granularityetting Azure credential

From the Azure Portal, navigate to `Storage Account` / `Containers`, and select your DGS container.  Please make note of the account name (should be `choruspilotstorage`) and the container name (should be a short name for your institution).  In the left menu, select `Settings` / `Shared access tokens`.  Create a new SAS token with `Read`, `Add`, `Create`, `Write`, and `List` enabled, and optionally `Delete` if you intend to use the same sas token for deletion later.  We do not need `Immutable Storage`.  Copy the SAS token string and save it in a secure location.  The SAS token will be used by the Upload Tool. 

If you are transferring files from a cloud account to CHoRUS, please refer to you institution's documentation to retrieve credentials for other storage clouds.  For a list of supported authentication mechanisms for each tested cloud providers, please see the `config.toml.template` file.

### Create or Update Manifest
To create or an update manifest, the required parameters are a manifest name, a `site-path`, and optionally the cloud credential if `site-path` is a cloud storage path.  Optionally, the type of data (`OMOP`, `Images`, `Waveforms`) to use to update manifest may be specified.   Multiple manifest updates may be performed before a data submission.

```
python chorus_upload_journal/journal_tools update
```


### Upload files

File upload follows the same pattern as manifest update. 

From local file system
```
python chorus_upload_journal/journal_tools upload 
```



<!-- 
TODO: file organization verification
TODO: change "manifest" to "journal"
illustration of the upload process and creation of the timestamp.
 -->
